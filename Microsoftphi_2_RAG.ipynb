{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d674ce-7c28-41a3-ba4d-3905cc9c1a83",
   "metadata": {},
   "source": [
    "# Microsoft Phi - 2 | RAG implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097bf0fd-d7d8-4f71-9003-b4253aeda8b6",
   "metadata": {},
   "source": [
    "Loading microsoft phi-2 in GPU, it takes arround 2GB of RAM ðŸš€."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2477fd06-3294-4dc0-90b5-e6e2617d3a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kavyansh/Workspace/RAG/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for microsoft/phi-2 contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/microsoft/phi-2.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n",
      "The repository for microsoft/phi-2 contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/microsoft/phi-2.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.19it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"microsoft/phi-2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype='float16',\n",
    "        bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babf201a-83ad-45c0-9b82-2d5ede7dffee",
   "metadata": {},
   "source": [
    "### Custom function for checking GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84493c87-2db1-43eb-bc40-281341c9b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def check_gpu_memory(): \n",
    "\n",
    "    # Check if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        gpu_memory = torch.cuda.get_device_properties(device).total_memory / 1e9 \n",
    "        print(f\"Total GPU Memory: {gpu_memory:.2f} GB\")\n",
    "        used_memory = torch.cuda.max_memory_allocated(device) / 1e9 \n",
    "        print(f\"Used GPU Memory: {used_memory:.2f} GB\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"GPU not available.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c86553-4785-4edb-bb38-f19b22f87356",
   "metadata": {},
   "source": [
    "## Chroma DB setup | Reading pdf's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccbea382-2ff7-43fd-a019-024528fc04ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98234b5e-448b-4f8b-85a8-c42a7220d34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.create_collection(name=\"my_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecfc6eeb-94b3-4013-b5b7-249dcdfa593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "reader = PdfReader(\"data.pdf\")\n",
    "number_of_pages = len(reader.pages)\n",
    "\n",
    "lis_of_docs = []\n",
    "lis_of_metadatas = []\n",
    "lis_of_ids = []\n",
    "\n",
    "def string_formater(passed_string):\n",
    "    words = passed_string.split()\n",
    "\n",
    "    formatted_string = ' '.join(words)\n",
    "    # print(formatted_string)\n",
    "    return formatted_string\n",
    "    \n",
    "c = 0\n",
    "for i in range(number_of_pages):\n",
    "    lis_of_docs.append(string_formater(reader.pages[c].extract_text()))\n",
    "    lis_of_metadatas.append({\"source\": \"source\" + str(c)})\n",
    "    lis_of_ids.append(str(c))\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59ffdcfa-8dc0-489a-a047-6624f175c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents = lis_of_docs,\n",
    "    metadatas = lis_of_metadatas,\n",
    "    ids=lis_of_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d15e6c-fd84-454c-8ef1-10d310a88818",
   "metadata": {},
   "source": [
    "This function will craft annswer and remove random strings, May be not work correctly, but it is working in most of cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aabd84e1-9a50-4c3b-a1de-886cb26e5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_formatter(input_string, word1, word2):\n",
    "    start_index = input_string.find(word1)\n",
    "    if word2 in input_string:\n",
    "        return input_string[start_index: input_string.find(word2)]\n",
    "    return input_string[start_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345e18d2-a755-4cc2-80d1-ddc772233e33",
   "metadata": {},
   "source": [
    "# Running on custom questions | Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32c3d471-f12b-4573-a722-133ed658cae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: The companies Kavyansh worked for are:\n",
      "\n",
      "- Solutions\n",
      "- Datopic\n",
      "- Hashedin by Deloitte\n",
      "- Aviz Networks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# question = \"What are the skills kavyansh has? form the sentance in list.\"\n",
    "question = \"Companies for those Kavyansh worked for?\"\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[question],\n",
    "    n_results=1\n",
    ")\n",
    "\n",
    "# print(results['documents'])\n",
    "\n",
    "context = \"\"\n",
    "for cnt in range(len(results['documents'])):\n",
    "    context += str(results['documents'][cnt]) + \"\\n\\n\"\n",
    "\n",
    "\n",
    "prompt = f\"\"\"Instruct: Response as Q&A Assitant and use data: {context} as reference. Question: {question} \\nOutput:\"\"\"\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
    "outputs = model.generate(**model_inputs, max_length=1000)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "# print(text)\n",
    "answer = answer_formatter(text, 'Output:','<|endoftext|>')\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9f0a430-8bbf-4664-bf8c-b0fe3763e766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 25.43 GB\n",
      "Used GPU Memory: 3.10 GB\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(check_gpu_memory())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d58de4c-9163-44b3-8159-854198b8b505",
   "metadata": {},
   "source": [
    "## Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d33825d-916f-4e62-a4f0-6da33dbb67ba",
   "metadata": {},
   "source": [
    "Make sure the pdfs, docs and files you are loading into vector DB in simple format. If they are very good quality images, videos. they will consume more memory and chances you will get CUDA - Out of Memory Error.\n",
    "\n",
    "Thanks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb831c-bcf9-4259-a230-2b19438629d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf0270b-84e0-4b92-834d-b5fa3451a81e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
